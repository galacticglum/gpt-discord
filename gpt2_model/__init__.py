"""GPT2 model for text generation."""
import re
import json
from pathlib import Path
from dataclasses import dataclass
from typing import Union, Optional, Tuple, List, Dict, Pattern, Any

import torch
import transformers
from transformers import (
    set_seed,
    AutoTokenizer,
    AutoModelWithLMHead,
    PreTrainedModel,
    PreTrainedTokenizer
)


def load_model(model_path: Union[str, Path],
               tokenizer_path: Optional[Union[str, Path]] = None,
               no_cuda: bool = False, quantize: bool = False) \
        -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
    """Load a pretrained language model and tokenizer.

    Args:
        model_path: The name of a standard pretrained model or a path
                    to a checkpoint for weights initialization.
        tokenizer_path: The name of a standard pretrained tokenizer or
                        a path to a checkpoint for weights initialization.
        no_cuda: Disable CUDA devices even when they are available.
        quantize: Indicates whether to quantize the loaded model.
    """
    # Setup device
    device = torch.device('cuda' if torch.cuda.is_available() and not no_cuda else 'cpu')
    if quantize and device != 'cpu':
        raise RuntimeError('Model quantization only available on CPU devices.')

    if tokenizer_path:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    elif model_path:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
    else:
        raise ValueError('Instantiating a new tokenizer from scratch is is not supported'
                         'However, it can be done from another script. Use the tokenizer_path '
                         'argument to provide it with the location of the script loading the tokenizer.')

    model = AutoModelWithLMHead.from_pretrained(model_path)
    if quantize:
        model = torch.quantization.quantize_dynamic(model, {
            # The layers to quantize
            torch.nn.Linear,
            torch.nn.Embedding,
            transformers.modeling_utils.Conv1D
        }, dtype=torch.qint8)

    return model.to(device), tokenizer


def _init_fp16(*args: list, opt_level: str = 'O1') -> Any:
    """Initializes the specified arguments with automated mixed percision (AMP)
    using NVIDIA Apex.
    """
    try:
        from apex import amp
    except ImportError:
        raise ImportError('Please install apex from https://www.github.com/nvidia/apex '
                          'to use fp16 inference.')

    return amp.initialize(*args, opt_level=opt_level)


def _verify_special_tokens(tokenizer: PreTrainedTokenizer, **kwargs: dict) -> None:
    """Verify that the given special tokens  (given as keyword arguments) are flagged
    as additional special tokens in the given tokenizer.
    """
    special_tokens = set(tokenizer.additional_special_tokens)
    for token_name, token_value in kwargs.items():
        if token_value in special_tokens:
            continue

        error = 'The {} token (\'{}\') is not marked as a special token.'.format(
            token_name, token_value
        )
        raise ValueError(error)


@dataclass
class Sample:
    """A single generated sample.

    Instance Attributes:
        - groups: A dict mapping the name of each capture group to its data.
        - raw_text: The raw output of the model (containing all special tokens).
    """
    groups: Dict[str, str]
    raw_text: str


def generate(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prompt: Optional[str] = None,
             samples: int = 1, top_k: int = 300, top_p: float = 1, temperature: float = 1.0,
             num_return_sequences: int = 10, max_iterations: int = 10, min_length: int = 10,
             max_length: int = 1024, fp16: bool = False, fp16_opt_level: float = 'O1',
             no_duplicates: bool = False, strict_regex_mapping: Optional[Union[str, Pattern]] = r'.*',
             special_tokens: Optional[Dict[str, str]] = None) -> List[Sample]:
    """Generate text from a transformer model with a language modelling head.

    Samples are generated by rolling out a search tree of possible messages from the prompt,
    and then filtering based on the cumulative probability of the returned tokens.

    Args:
        model: The transformers.PreTrainedModel to use for inference.
        tokenizer: The transformers.PreTrainedTokenizer to use to encode decode text/token
                   sequences.
        prompt: A prompt for the model.
        samples: The number of samples to generate.
        top_k: The number of highest probability in-vocabulary tokens to keep.
               Must be between 1 and infinity.
        top_p: The cumulative probability of parameter highest probability in-vocabulary
               tokens to keep for nucleas sampling. Must be between 0 and 1 (inclusive).
        temperature: Sampling temperature.
        num_return_sequences: The number of sequences to return per iteration.
        max_iterations: Maximum number of iterations. If -1, there is no maximum number of
                        iterations (the function will wait until all samples have been generated).
                        Warning: This is likely to be very slow, and thread-blocking!
        min_length: Minimum number of tokens to generate in a single iteration.
        max_length: Maximum number of tokens to generate in a single iteration.
        fp16: Whether to use 16-bit (mixed) precision floats (note: requires NVIDIA Apex!).
        fp16_opt_level: Apex AMP optimization level. See https://nvidia.github.io/apex/amp.html.
        no_duplicates: Don't generate any duplicate records.
        strict_regex_mapping: A regex pattern for splitting the model output into capture groups.
                              If given as a string, then it can contain named string formatting arguments
                              corresponding to the names of special tokens from the tokenizer (use
                              "bos_token" for the beginning of sequence token, "eos_token" for the
                              end of sequence token, and "pad_token" for the padding token).
        special_tokens: A dict mapping the name of an additional special token to its value.
    """
    if fp16:
        model = _init_fp16(model, opt_level=fp16_opt_level)

    # Verify additional special tokens
    if special_tokens is None:
        special_tokens = dict()
    _verify_special_tokens(tokenizer, **special_tokens)

    # If not prompt is specified, the default is the BOS token.
    prompt = prompt or tokenizer.bos_token
    # Encode the prompt using the tokenizer
    prompt_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)

    if isinstance(strict_regex_mapping, str):
        special_tokens_map = tokenizer.special_tokens_map
        special_tokens_map.pop('additional_special_tokens', None)
        for name, token in special_tokens.items():
            special_tokens_map[name] = token
        strict_regex_mapping = strict_regex_mapping.format(**special_tokens_map)

    # Compile pattern
    strict_regex_mapping = re.compile(strict_regex_mapping)

    results = []
    visited = set()
    current_iteration = 0
    while len(results) < samples:
        if max_iterations != -1 and current_iteration > max_iterations:
            break

        current_iteration += 1
        remaining_samples = samples - len(results)
        # Multiply by some 'arbitrary' scale factor to pad the next attempt in case there are
        # any failed attempts. We use 1.5 as an approximation under the assumption that 50% of
        # the samples in iteration are failed (this is an overestimation for safety).
        num_return_sequences = min(int(remaining_samples * 1.5), num_return_sequences)
        output = model.generate(
            prompt_ids,
            bos_token_id=tokenizer.bos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            top_k=top_k,
            top_p=top_p,
            num_return_sequences=num_return_sequences,
            min_length=min_length,
            max_length=max_length,
            do_sample=True,
            temperature=temperature
        )

        for i in range(output.size()[0]):
            if len(results) >= samples:
                break
            raw_text = tokenizer.decode(output[i, :].tolist())
            match = strict_regex_mapping.match(raw_text)
            if not match:
                continue

            groups = match.groupdict()
            # Check if generated sequence has missing matching groups.
            if any(value is None for value in groups.values()):
                continue
            # Strip all match groups of trailing whitespace
            groups = {key: value.strip() for key, value in groups.items()}
            if no_duplicates:
                groups_id = json.dumps(groups, sort_keys=True)
                if groups_id in visited:
                    continue
                visited.add(groups_id)
            results.append(Sample(groups, raw_text))
        return results
